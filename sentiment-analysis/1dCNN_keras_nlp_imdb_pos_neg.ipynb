{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, Flatten, Dense, Conv1D, MaxPooling1D, GlobalMaxPooling1D\n",
    "from keras import Model, layers\n",
    "from keras import Input\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "import numpy as np\n",
    "from keras.optimizers import RMSprop\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import pickle as pk\n",
    "\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "UnicodeDecodeError",
     "evalue": "'charmap' codec can't decode byte 0x9d in position 1757: character maps to <undefined>",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnicodeDecodeError\u001b[0m                        Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-12-b0311fc44eec>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     12\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mfname\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'.txt'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m             \u001b[0mf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdir_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 14\u001b[1;33m             \u001b[0mtexts\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     15\u001b[0m             \u001b[0mf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mlabel_type\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'neg'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\deeplearning\\lib\\encodings\\cp1252.py\u001b[0m in \u001b[0;36mdecode\u001b[1;34m(self, input, final)\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[1;32mclass\u001b[0m \u001b[0mIncrementalDecoder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcodecs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mIncrementalDecoder\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mdecode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfinal\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 23\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mcodecs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcharmap_decode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0merrors\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdecoding_table\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     24\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[1;32mclass\u001b[0m \u001b[0mStreamWriter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mCodec\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mcodecs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mStreamWriter\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mUnicodeDecodeError\u001b[0m: 'charmap' codec can't decode byte 0x9d in position 1757: character maps to <undefined>"
     ]
    }
   ],
   "source": [
    "# get data or pickle files if there\n",
    "\n",
    "imdb_dir = '../../../data/aclImdb/test'\n",
    "# train_dir = os.path.join(imdb_dir, 'train')\n",
    "labels = list()\n",
    "texts = list()\n",
    "\n",
    "# Processing the labels of the raw IMDB data\n",
    "for label_type in ['neg', 'pos']:\n",
    "    dir_name = os.path.join(imdb_dir, label_type)\n",
    "    for fname in os.listdir(dir_name):\n",
    "        if fname[-4:] == '.txt':\n",
    "            f = open(os.path.join(dir_name, fname))\n",
    "            texts.append(f.read())\n",
    "            f.close()\n",
    "            if label_type == 'neg':\n",
    "                labels.append(0)\n",
    "            else:\n",
    "                labels.append(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenizing the data\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "import numpy as np\n",
    "from keras.optimizers import RMSprop\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenizing the data\n",
    "\n",
    "# cut off reviews after 500 words\n",
    "max_len = 500 \n",
    "# train on 10000 samples\n",
    "training_samples = 10000\n",
    " # validate on 10000 samples \n",
    "validation_samples = 10000\n",
    "# consider only the top 10000 words\n",
    "max_words = 10000 \n",
    "\n",
    "tokenizer_path = 'tokenizer'\n",
    "# import tokenizer with the consideration for only the top 500 words\n",
    "tokenizer = Tokenizer(num_words=max_words) \n",
    "# fit the tokenizer on the texts\n",
    "tokenizer.fit_on_texts(texts) \n",
    "# convert the texts to sequences\n",
    "sequences = tokenizer.texts_to_sequences(texts) \n",
    "\n",
    "# save the tokenizer\n",
    "with open(os.path.join(tokenizer_path, 'tokenizer_m1.pickle'), 'wb') as handle:\n",
    "    pk.dump(tokenizer, handle, protocol=pk.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens. ' % len(word_index))\n",
    "\n",
    " # pad the sequence to the required length to ensure uniformity\n",
    "data = pad_sequences(sequences, maxlen=max_len)\n",
    "print('Data Shape: {}'.format(data.shape))\n",
    "\n",
    "labels = np.asarray(labels)\n",
    "print(\"Shape of data tensor: \", data.shape)\n",
    "print(\"Shape of label tensor: \", labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the data into training and validation set but before that shuffle it first\n",
    "indices = np.arange(data.shape[0])\n",
    "np.random.shuffle(indices)\n",
    "data = data[indices]\n",
    "labels = labels[indices]\n",
    "\n",
    "x_train = data[:training_samples]\n",
    "y_train = labels[:training_samples]\n",
    "x_val = data[training_samples:training_samples + validation_samples]\n",
    "y_val = labels[training_samples:training_samples + validation_samples]\n",
    "\n",
    "# test_data\n",
    "x_test = data[training_samples+validation_samples:]\n",
    "y_test = labels[training_samples+validation_samples:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model definition\n",
    "\n",
    "callback_list = [\n",
    "    keras.callbacks.EarlyStopping(\n",
    "        patience=1,\n",
    "        monitor='acc',\n",
    "    ),\n",
    "    \n",
    "    keras.callbacks.TensorBoard(\n",
    "        log_dir='log_dir_m1',\n",
    "        histogram_freq=1,\n",
    "        embeddings_freq=1,\n",
    "    ),\n",
    "\n",
    "    keras.callbacks.ModelCheckpoint(\n",
    "        monitor='val_loss',\n",
    "        save_best_only=True,\n",
    "        filepath='model/movie_sentiment_m1.h5',\n",
    "    ),\n",
    "\n",
    "    keras.callbacks.ReduceLROnPlateau(\n",
    "        patience=1,\n",
    "        factor=0.1,\n",
    "    )\n",
    "]\n",
    "\n",
    "# layer developing\n",
    "text_input_layer = Input(shape=(max_len,))\n",
    "embedding_layer = Embedding(max_words, 50)(text_input_layer)\n",
    "text_layer = Conv1D(256, 3, activation='relu')(embedding_layer)\n",
    "text_layer = MaxPooling1D(3)(text_layer)\n",
    "text_layer = Conv1D(256, 3, activation='relu')(text_layer)\n",
    "text_layer = MaxPooling1D(3)(text_layer)\n",
    "text_layer = Conv1D(256, 3, activation='relu')(text_layer)\n",
    "text_layer = MaxPooling1D(3)(text_layer)\n",
    "text_layer = Conv1D(256, 3, activation='relu')(text_layer)\n",
    "text_layer = MaxPooling1D(3)(text_layer)\n",
    "text_layer = Conv1D(256, 3, activation='relu')(text_layer)\n",
    "text_layer = MaxPooling1D(3)(text_layer)\n",
    "text_layer = GlobalMaxPooling1D()(text_layer)\n",
    "text_layer = Dense(256, activation='relu')(text_layer)\n",
    "output_layer = Dense(1, activation='sigmoid')(text_layer)\n",
    "model = Model(text_input_layer, output_layer)\n",
    "model.summary()\n",
    "model.compile(optimizer=RMSprop(lr=0.001),\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['acc'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# multi-input test\n",
    "history = model.fit(x_train, y_train, epochs=50, batch_size=128, callbacks=callback_list,\n",
    "                    validation_data=(x_val, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting the results\n",
    "acc = history.history.get('acc')\n",
    "val_acc = history.history.get('val_acc')\n",
    "loss = history.history.get('loss')\n",
    "val_loss = history.history.get('val_loss')\n",
    "\n",
    "epochs = range(1, len(acc) + 1)\n",
    "plt.plot(epochs, acc, 'bo', label='Training Acc')\n",
    "plt.plot(epochs, val_acc, 'b', label='Validation Acc')\n",
    "plt.title('Training and Validation Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.figure()\n",
    "\n",
    "plt.plot(epochs, loss, 'bo', label=\"Training Loss\")\n",
    "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
    "plt.title('Training and Validation loss')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[235, 671, 692, 954, 104, 328,  58,   5, 927,   4],\n",
       "       [422, 771, 353, 862, 858, 993, 544,  10, 836, 456],\n",
       "       [889, 478, 535, 616, 101, 234, 915, 803, 920, 909],\n",
       "       [547,  16, 367, 548, 843, 926,  89, 666, 769,  65],\n",
       "       [408, 905, 632, 927, 137, 237, 206, 509, 484,  78],\n",
       "       [373, 607, 963, 924, 179, 806, 861, 815,  36, 867],\n",
       "       [252, 529, 824, 704, 821, 433, 457, 753, 200, 920],\n",
       "       [387, 348, 479, 495, 634, 569, 907, 149, 891, 315],\n",
       "       [ 94, 212, 601, 445, 172, 279, 380, 228, 376, 738],\n",
       "       [759, 569, 592, 354, 610,  50, 737, 418, 695, 567],\n",
       "       [404, 119, 452, 281, 204, 559, 697,  31, 399, 313],\n",
       "       [ 86, 980,  73, 548, 974, 183,  22,  39, 172, 760],\n",
       "       [172, 646, 824, 252,  40, 671, 300, 871, 302, 990],\n",
       "       [459,  81, 803, 365, 588,  60, 223, 377, 214, 754],\n",
       "       [841, 840, 107, 825, 604, 366, 863, 126,   0, 237],\n",
       "       [699, 905, 914, 792, 950, 667, 381, 765, 562, 907],\n",
       "       [437, 959, 157, 763, 994, 670, 262, 902, 501, 824],\n",
       "       [539,  17, 170, 876, 465, 536, 549, 533, 523,  19],\n",
       "       [715, 686, 492,  64,  44, 267,  23, 863,  80, 410],\n",
       "       [631, 732, 276, 180, 244, 102,  84, 597, 681,  88],\n",
       "       [961,  93, 402,  45, 609, 998,  64, 406, 285, 505],\n",
       "       [771, 217,  28, 132, 863, 872, 889, 347, 333,  22],\n",
       "       [  3, 405, 992, 584, 946, 426, 258, 198, 673, 735],\n",
       "       [548, 285, 301, 788, 949, 563, 481, 932, 167, 337],\n",
       "       [973, 370, 713, 596, 177, 169, 606, 275, 494, 507],\n",
       "       [801, 731, 849, 356, 465, 995, 981, 676, 267, 137],\n",
       "       [464, 653, 796, 104, 860, 772, 916, 385, 705, 790],\n",
       "       [138, 947, 340, 163, 320, 196, 762, 208, 571, 197],\n",
       "       [217,   9, 780, 494, 476, 807, 543, 569, 769, 704],\n",
       "       [310, 647, 188, 216, 819, 518,  18,   5, 758, 427],\n",
       "       [826, 112, 593, 783, 830, 114, 976, 652,  16,  78],\n",
       "       [861, 567, 419, 938, 762, 465, 946, 408, 984, 509]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_array = np.random.randint(1000, size=(32, 10))\n",
    "input_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 1. 0.]\n",
      " [1. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "category_indices = [0, 1, 2, 2, 1, 0]\n",
    "unique_category_count = 3\n",
    "inputs = tf.one_hot(category_indices, unique_category_count)\n",
    "print(inputs.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[38, 19], [40, 11], [29, 45], [20, 11], [12], [10], [23, 45], [33, 40], [23, 11], [35, 39, 19, 38]]\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.text import one_hot\n",
    "docs = ['Well done!',\n",
    "\t\t'Good work',\n",
    "\t\t'Great effort',\n",
    "\t\t'nice work',\n",
    "\t\t'Excellent!',\n",
    "\t\t'Weak',\n",
    "\t\t'Poor effort!',\n",
    "\t\t'not good',\n",
    "\t\t'poor work',\n",
    "\t\t'Could have done better.']\n",
    "# define class labels\n",
    "vocab_size = 50\n",
    "encoded_docs = [one_hot(d, vocab_size) for d in docs]\n",
    "print(encoded_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
