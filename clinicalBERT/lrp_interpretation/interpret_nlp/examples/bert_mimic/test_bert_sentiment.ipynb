{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bert Sentiment Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../../../\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from IPython.core.display import display, HTML\n",
    "from transformers import BertTokenizer\n",
    "\n",
    "from modules.lrp_bert_modules import LRPBertForSequenceClassification\n",
    "\n",
    "from visualization.heatmap import html_heatmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model...\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "# print(\"Loading model...\")\n",
    "# config_path = \"bert-sst-config.pt\"\n",
    "# state_dict_path = \"bert-sst.pt\"\n",
    "\n",
    "# tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "# model = LRPBertForSequenceClassification(torch.load(config_path))\n",
    "# model.load_state_dict(torch.load(state_dict_path))\n",
    "# model.eval()\n",
    "# print(\"Done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CIURRENTLY THIS IS WORKING WITH ORIGINAL MODEL FROM CLINICAL BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "ename": "UnpicklingError",
     "evalue": "invalid load key, '{'.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnpicklingError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-40-27fb491131eb>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;31m#load 1 layer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[0mmodel1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mLRPBertForSequenceClassification\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel_1layer_discharge_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_labels\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 14\u001b[1;33m \u001b[0mmodel2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mLRPBertForSequenceClassification\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel_1layer_discharge_config_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     15\u001b[0m \u001b[0mmodel2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel_1layer_discharge_model_path\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mmap_location\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'cpu'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\deeplearning\\lib\\site-packages\\torch\\serialization.py\u001b[0m in \u001b[0;36mload\u001b[1;34m(f, map_location, pickle_module, **pickle_load_args)\u001b[0m\n\u001b[0;32m    591\u001b[0m                     \u001b[1;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjit\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    592\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0m_load\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mopened_zipfile\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmap_location\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpickle_module\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mpickle_load_args\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 593\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0m_legacy_load\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mopened_file\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmap_location\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpickle_module\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mpickle_load_args\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    594\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    595\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\deeplearning\\lib\\site-packages\\torch\\serialization.py\u001b[0m in \u001b[0;36m_legacy_load\u001b[1;34m(f, map_location, pickle_module, **pickle_load_args)\u001b[0m\n\u001b[0;32m    761\u001b[0m             \"functionality.\".format(type(f)))\n\u001b[0;32m    762\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 763\u001b[1;33m     \u001b[0mmagic_number\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpickle_module\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mpickle_load_args\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    764\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mmagic_number\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mMAGIC_NUMBER\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    765\u001b[0m         \u001b[1;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Invalid magic number; corrupt file?\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mUnpicklingError\u001b[0m: invalid load key, '{'."
     ]
    }
   ],
   "source": [
    "#load original model from clinicalBERT\n",
    "# config_path = \"./bert_config.json\"\n",
    "# model_path = \"./pytorch_model.bin\"\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "\n",
    "model_1layer_discharge_config_path = \"../../../../model/newest_ClinicalBERTPretained_discharge_210421/config.json\"\n",
    "model_1layer_discharge_model_path = \"../../../../model/newest_ClinicalBERTPretained_discharge_210421/pytorch_model.bin\"\n",
    "\n",
    "model_3layer_discharge_path = \"../../../../model/newest_3layerClinicalBERTPretained_discharge_210421/\"\n",
    "\n",
    "#load 1 layer \n",
    "model1 = LRPBertForSequenceClassification.from_pretrained(model_1layer_discharge_path, num_labels = 1)\n",
    "model2 = LRPBertForSequenceClassification(torch.load(model_1layer_discharge_config_path))\n",
    "model2.load_state_dict(torch.load(model_1layer_discharge_model_path,map_location='cpu'))\n",
    "\n",
    "model3 = LRPBertForSequenceClassification.from_pretrained(model_3layer_discharge_path, num_labels = 1)\n",
    "# model = LRPBertForSequenceClassification.from_pretrained(torch.load(model_path))\n",
    "\n",
    "# model = torch.load(model_path)\n",
    "print(model1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set up example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_example = \"It's a lovely film with wonderful performances by Buy and \" \\\n",
    "#                \"Accorsi.\"\n",
    "\n",
    "test_example = ' he, has, experienced, acute, on, chronic, diastolic, heart, failure, in the setting of volume overload due to his sepsis.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' he, has, experienced, acute, on, chronic, diastolic, heart, failure, in the setting of volume overload due to his sepsis.'"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run normal forward pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.96026367\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "inputs = tokenizer(test_example, return_tensors=\"pt\")\n",
    "logits = model(**inputs).logits.squeeze()\n",
    "\n",
    "m = nn.Sigmoid()\n",
    "logits = torch.squeeze(m(logits)).detach().cpu().numpy()\n",
    "\n",
    "print(logits)\n",
    "# classes = [\"<unk>\", \"positive\", \"negative\", \"neutral\"]\n",
    "classes = [\"not_admitted\", \"admitted\"]\n",
    "\n",
    "# print(\"Logit Scores:\")\n",
    "# for c, score in zip(classes, logits):\n",
    "#     print(\"{}: {}\".format(c, score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run attribution forward pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attr Forward Pass Output:\n",
      "[[3.005818]]\n"
     ]
    }
   ],
   "source": [
    "inputs = tokenizer(test_example, return_tensors=\"pt\")\n",
    "model.attr()\n",
    "output = model(**inputs)\n",
    "\n",
    "print(\"Attr Forward Pass Output:\")\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run LRP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LRP Scores:\n",
      "he: 0.046435107968599586\n",
      "has: 0.006323041011875523\n",
      "experienced: -0.12908340886646283\n",
      "acute: -0.36771864139854116\n",
      "on: -0.11877602452014005\n",
      "chronic: 0.18068827691213424\n",
      "dia: 0.16323471084273566\n",
      "##sto: -0.30414113223838124\n",
      "##lic: 0.18059337185346486\n",
      "heart: -0.13958726169712377\n",
      "failure: 0.1647037468879956\n",
      "in: -0.07315569959288457\n",
      "the: 0.159346919019919\n",
      "setting: 0.06668113958085745\n",
      "of: -0.06719082451758973\n",
      "volume: -0.08085323940011024\n",
      "over: -0.06246023042943781\n",
      "##load: -0.04087739198209185\n",
      "due: -0.09033322024342459\n",
      "to: -0.16731615607760178\n",
      "his: 0.044149933816612585\n",
      "sep: -0.706693439985873\n",
      "##sis: -0.7826203551596715\n",
      ".: 0.04353143368002793\n",
      "Relevance of word embeddings:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:#dedbda;font-weight:bold;background-color: #ffffff;padding-top: 15px;padding-bottom: 15px;\"> he </span> <span style=\"color:#dcdcdd;font-weight:bold;background-color: #ffffff;padding-top: 15px;padding-bottom: 15px;\"> has </span> <span style=\"color:#d8dbe1;font-weight:bold;background-color: #ffffff;padding-top: 15px;padding-bottom: 15px;\"> experienced </span> <span style=\"color:#b6cef9;font-weight:bold;background-color: #ffffff;padding-top: 15px;padding-bottom: 15px;\"> acute </span> <span style=\"color:#d7dbe2;font-weight:bold;background-color: #ffffff;padding-top: 15px;padding-bottom: 15px;\"> on </span> <span style=\"color:#d2dae7;font-weight:bold;background-color: #ffffff;padding-top: 15px;padding-bottom: 15px;\"> chronic </span> <span style=\"color:#e9d4c9;font-weight:bold;background-color: #ffffff;padding-top: 15px;padding-bottom: 15px;\"> dia </span> <span style=\"color:#b8cff8;font-weight:bold;background-color: #ffffff;padding-top: 15px;padding-bottom: 15px;\"> ##sto </span> <span style=\"color:#e1dad6;font-weight:bold;background-color: #ffffff;padding-top: 15px;padding-bottom: 15px;\"> ##lic </span> <span style=\"color:#d1dae8;font-weight:bold;background-color: #ffffff;padding-top: 15px;padding-bottom: 15px;\"> heart </span> <span style=\"color:#e5d8d0;font-weight:bold;background-color: #ffffff;padding-top: 15px;padding-bottom: 15px;\"> failure </span> <span style=\"color:#d8dbe1;font-weight:bold;background-color: #ffffff;padding-top: 15px;padding-bottom: 15px;\"> in </span> <span style=\"color:#e8d5ca;font-weight:bold;background-color: #ffffff;padding-top: 15px;padding-bottom: 15px;\"> the </span> <span style=\"color:#e5d8d0;font-weight:bold;background-color: #ffffff;padding-top: 15px;padding-bottom: 15px;\"> setting </span> <span style=\"color:#dadcdf;font-weight:bold;background-color: #ffffff;padding-top: 15px;padding-bottom: 15px;\"> of </span> <span style=\"color:#e1dad6;font-weight:bold;background-color: #ffffff;padding-top: 15px;padding-bottom: 15px;\"> volume </span> <span style=\"color:#d9dce0;font-weight:bold;background-color: #ffffff;padding-top: 15px;padding-bottom: 15px;\"> over </span> <span style=\"color:#c7d6f0;font-weight:bold;background-color: #ffffff;padding-top: 15px;padding-bottom: 15px;\"> ##load </span> <span style=\"color:#d9dce0;font-weight:bold;background-color: #ffffff;padding-top: 15px;padding-bottom: 15px;\"> due </span> <span style=\"color:#d6dbe4;font-weight:bold;background-color: #ffffff;padding-top: 15px;padding-bottom: 15px;\"> to </span> <span style=\"color:#e5d8d0;font-weight:bold;background-color: #ffffff;padding-top: 15px;padding-bottom: 15px;\"> his </span> <span style=\"color:#b30326;font-weight:bold;background-color: #ffffff;padding-top: 15px;padding-bottom: 15px;\"> sep </span> <span style=\"color:#c1d4f4;font-weight:bold;background-color: #ffffff;padding-top: 15px;padding-bottom: 15px;\"> ##sis </span> <span style=\"color:#e1dad6;font-weight:bold;background-color: #ffffff;padding-top: 15px;padding-bottom: 15px;\"> . </span> "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Relevance of positional embeddings:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:#dcdcdd;font-weight:bold;background-color: #ffffff;padding-top: 15px;padding-bottom: 15px;\"> he </span> <span style=\"color:#d8dbe1;font-weight:bold;background-color: #ffffff;padding-top: 15px;padding-bottom: 15px;\"> has </span> <span style=\"color:#d8dbe1;font-weight:bold;background-color: #ffffff;padding-top: 15px;padding-bottom: 15px;\"> experienced </span> <span style=\"color:#dadcdf;font-weight:bold;background-color: #ffffff;padding-top: 15px;padding-bottom: 15px;\"> acute </span> <span style=\"color:#dbdcde;font-weight:bold;background-color: #ffffff;padding-top: 15px;padding-bottom: 15px;\"> on </span> <span style=\"color:#d9dce0;font-weight:bold;background-color: #ffffff;padding-top: 15px;padding-bottom: 15px;\"> chronic </span> <span style=\"color:#cdd9ec;font-weight:bold;background-color: #ffffff;padding-top: 15px;padding-bottom: 15px;\"> dia </span> <span style=\"color:#d6dbe4;font-weight:bold;background-color: #ffffff;padding-top: 15px;padding-bottom: 15px;\"> ##sto </span> <span style=\"color:#dedbda;font-weight:bold;background-color: #ffffff;padding-top: 15px;padding-bottom: 15px;\"> ##lic </span> <span style=\"color:#dedbda;font-weight:bold;background-color: #ffffff;padding-top: 15px;padding-bottom: 15px;\"> heart </span> <span style=\"color:#d9dce0;font-weight:bold;background-color: #ffffff;padding-top: 15px;padding-bottom: 15px;\"> failure </span> <span style=\"color:#dadcdf;font-weight:bold;background-color: #ffffff;padding-top: 15px;padding-bottom: 15px;\"> in </span> <span style=\"color:#ecd2c4;font-weight:bold;background-color: #ffffff;padding-top: 15px;padding-bottom: 15px;\"> the </span> <span style=\"color:#d8dbe1;font-weight:bold;background-color: #ffffff;padding-top: 15px;padding-bottom: 15px;\"> setting </span> <span style=\"color:#d1dae8;font-weight:bold;background-color: #ffffff;padding-top: 15px;padding-bottom: 15px;\"> of </span> <span style=\"color:#d8dbe1;font-weight:bold;background-color: #ffffff;padding-top: 15px;padding-bottom: 15px;\"> volume </span> <span style=\"color:#e0dad7;font-weight:bold;background-color: #ffffff;padding-top: 15px;padding-bottom: 15px;\"> over </span> <span style=\"color:#dfdbd9;font-weight:bold;background-color: #ffffff;padding-top: 15px;padding-bottom: 15px;\"> ##load </span> <span style=\"color:#d3dbe6;font-weight:bold;background-color: #ffffff;padding-top: 15px;padding-bottom: 15px;\"> due </span> <span style=\"color:#d6dbe4;font-weight:bold;background-color: #ffffff;padding-top: 15px;padding-bottom: 15px;\"> to </span> <span style=\"color:#d9dce0;font-weight:bold;background-color: #ffffff;padding-top: 15px;padding-bottom: 15px;\"> his </span> <span style=\"color:#b30326;font-weight:bold;background-color: #ffffff;padding-top: 15px;padding-bottom: 15px;\"> sep </span> <span style=\"color:#cdd9ec;font-weight:bold;background-color: #ffffff;padding-top: 15px;padding-bottom: 15px;\"> ##sis </span> <span style=\"color:#dddcdb;font-weight:bold;background-color: #ffffff;padding-top: 15px;padding-bottom: 15px;\"> . </span> "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Relevance of type embeddings:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:#dddcdb;font-weight:bold;background-color: #ffffff;padding-top: 15px;padding-bottom: 15px;\"> he </span> <span style=\"color:#dfdbd9;font-weight:bold;background-color: #ffffff;padding-top: 15px;padding-bottom: 15px;\"> has </span> <span style=\"color:#d0dae9;font-weight:bold;background-color: #ffffff;padding-top: 15px;padding-bottom: 15px;\"> experienced </span> <span style=\"color:#edcfc0;font-weight:bold;background-color: #ffffff;padding-top: 15px;padding-bottom: 15px;\"> acute </span> <span style=\"color:#d0dae9;font-weight:bold;background-color: #ffffff;padding-top: 15px;padding-bottom: 15px;\"> on </span> <span style=\"color:#dbdcde;font-weight:bold;background-color: #ffffff;padding-top: 15px;padding-bottom: 15px;\"> chronic </span> <span style=\"color:#cfd9ea;font-weight:bold;background-color: #ffffff;padding-top: 15px;padding-bottom: 15px;\"> dia </span> <span style=\"color:#bed3f5;font-weight:bold;background-color: #ffffff;padding-top: 15px;padding-bottom: 15px;\"> ##sto </span> <span style=\"color:#e1dad6;font-weight:bold;background-color: #ffffff;padding-top: 15px;padding-bottom: 15px;\"> ##lic </span> <span style=\"color:#e8d5ca;font-weight:bold;background-color: #ffffff;padding-top: 15px;padding-bottom: 15px;\"> heart </span> <span style=\"color:#dcdcdd;font-weight:bold;background-color: #ffffff;padding-top: 15px;padding-bottom: 15px;\"> failure </span> <span style=\"color:#e7d6cd;font-weight:bold;background-color: #ffffff;padding-top: 15px;padding-bottom: 15px;\"> in </span> <span style=\"color:#e0dad7;font-weight:bold;background-color: #ffffff;padding-top: 15px;padding-bottom: 15px;\"> the </span> <span style=\"color:#c9d7ee;font-weight:bold;background-color: #ffffff;padding-top: 15px;padding-bottom: 15px;\"> setting </span> <span style=\"color:#d7dbe2;font-weight:bold;background-color: #ffffff;padding-top: 15px;padding-bottom: 15px;\"> of </span> <span style=\"color:#cad8ee;font-weight:bold;background-color: #ffffff;padding-top: 15px;padding-bottom: 15px;\"> volume </span> <span style=\"color:#dcdcdd;font-weight:bold;background-color: #ffffff;padding-top: 15px;padding-bottom: 15px;\"> over </span> <span style=\"color:#f6bda4;font-weight:bold;background-color: #ffffff;padding-top: 15px;padding-bottom: 15px;\"> ##load </span> <span style=\"color:#d0dae9;font-weight:bold;background-color: #ffffff;padding-top: 15px;padding-bottom: 15px;\"> due </span> <span style=\"color:#cfd9ea;font-weight:bold;background-color: #ffffff;padding-top: 15px;padding-bottom: 15px;\"> to </span> <span style=\"color:#a2c0fe;font-weight:bold;background-color: #ffffff;padding-top: 15px;padding-bottom: 15px;\"> his </span> <span style=\"color:#3a4cc0;font-weight:bold;background-color: #ffffff;padding-top: 15px;padding-bottom: 15px;\"> sep </span> <span style=\"color:#e4d8d1;font-weight:bold;background-color: #ffffff;padding-top: 15px;padding-bottom: 15px;\"> ##sis </span> <span style=\"color:#e3d9d3;font-weight:bold;background-color: #ffffff;padding-top: 15px;padding-bottom: 15px;\"> . </span> "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Relevance of combined embeddings:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:#e4d8d1;font-weight:bold;background-color: #ffffff;padding-top: 15px;padding-bottom: 15px;\"> he </span> <span style=\"color:#dedbda;font-weight:bold;background-color: #ffffff;padding-top: 15px;padding-bottom: 15px;\"> has </span> <span style=\"color:#c5d5f2;font-weight:bold;background-color: #ffffff;padding-top: 15px;padding-bottom: 15px;\"> experienced </span> <span style=\"color:#91b3fe;font-weight:bold;background-color: #ffffff;padding-top: 15px;padding-bottom: 15px;\"> acute </span> <span style=\"color:#c7d6f0;font-weight:bold;background-color: #ffffff;padding-top: 15px;padding-bottom: 15px;\"> on </span> <span style=\"color:#f3c6b0;font-weight:bold;background-color: #ffffff;padding-top: 15px;padding-bottom: 15px;\"> chronic </span> <span style=\"color:#f2c9b5;font-weight:bold;background-color: #ffffff;padding-top: 15px;padding-bottom: 15px;\"> dia </span> <span style=\"color:#a0bffe;font-weight:bold;background-color: #ffffff;padding-top: 15px;padding-bottom: 15px;\"> ##sto </span> <span style=\"color:#f3c6b0;font-weight:bold;background-color: #ffffff;padding-top: 15px;padding-bottom: 15px;\"> ##lic </span> <span style=\"color:#c3d5f2;font-weight:bold;background-color: #ffffff;padding-top: 15px;padding-bottom: 15px;\"> heart </span> <span style=\"color:#f2c9b5;font-weight:bold;background-color: #ffffff;padding-top: 15px;padding-bottom: 15px;\"> failure </span> <span style=\"color:#d0dae9;font-weight:bold;background-color: #ffffff;padding-top: 15px;padding-bottom: 15px;\"> in </span> <span style=\"color:#f2c9b5;font-weight:bold;background-color: #ffffff;padding-top: 15px;padding-bottom: 15px;\"> the </span> <span style=\"color:#e7d6cd;font-weight:bold;background-color: #ffffff;padding-top: 15px;padding-bottom: 15px;\"> setting </span> <span style=\"color:#d1dae8;font-weight:bold;background-color: #ffffff;padding-top: 15px;padding-bottom: 15px;\"> of </span> <span style=\"color:#ced9eb;font-weight:bold;background-color: #ffffff;padding-top: 15px;padding-bottom: 15px;\"> volume </span> <span style=\"color:#d1dae8;font-weight:bold;background-color: #ffffff;padding-top: 15px;padding-bottom: 15px;\"> over </span> <span style=\"color:#d6dbe4;font-weight:bold;background-color: #ffffff;padding-top: 15px;padding-bottom: 15px;\"> ##load </span> <span style=\"color:#cdd9ec;font-weight:bold;background-color: #ffffff;padding-top: 15px;padding-bottom: 15px;\"> due </span> <span style=\"color:#bdd2f6;font-weight:bold;background-color: #ffffff;padding-top: 15px;padding-bottom: 15px;\"> to </span> <span style=\"color:#e4d8d1;font-weight:bold;background-color: #ffffff;padding-top: 15px;padding-bottom: 15px;\"> his </span> <span style=\"color:#4860d1;font-weight:bold;background-color: #ffffff;padding-top: 15px;padding-bottom: 15px;\"> sep </span> <span style=\"color:#3a4cc0;font-weight:bold;background-color: #ffffff;padding-top: 15px;padding-bottom: 15px;\"> ##sis </span> <span style=\"color:#e4d8d1;font-weight:bold;background-color: #ffffff;padding-top: 15px;padding-bottom: 15px;\"> . </span> "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokens = tokenizer.tokenize(test_example)\n",
    "rel_y = np.zeros(output.shape)\n",
    "rel_y[:, 0] = output[:, 0]\n",
    "rel_word, rel_pos, rel_type, rel_embed = model.attr_backward(rel_y, eps=.1)\n",
    "rel_word = np.sum(rel_word[0, 1:-1], -1)\n",
    "rel_pos = np.sum(rel_pos[0, 1:-1], -1)\n",
    "rel_type = np.sum(rel_type[0, 1:-1], -1)\n",
    "rel_embed = np.sum(rel_embed[0, 1:-1], -1)\n",
    "\n",
    "print(\"LRP Scores:\")\n",
    "for t, s in zip(tokens, rel_embed):\n",
    "    print(t, s, sep=\": \")\n",
    "    \n",
    "print(\"Relevance of word embeddings:\")\n",
    "display(HTML(html_heatmap(tokens, list(rel_word))))\n",
    "\n",
    "print(\"Relevance of positional embeddings:\")\n",
    "display(HTML(html_heatmap(tokens, list(rel_pos))))\n",
    "\n",
    "print(\"Relevance of type embeddings:\")\n",
    "display(HTML(html_heatmap(tokens, list(rel_type))))\n",
    "\n",
    "print(\"Relevance of combined embeddings:\")\n",
    "display(HTML(html_heatmap(tokens, list(rel_embed))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "index 1 is out of bounds for axis 1 with size 1",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-55-52e7277c4f89>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mrel_y\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m: index 1 is out of bounds for axis 1 with size 1"
     ]
    }
   ],
   "source": [
    "def get_LRP_derived_token_scores():\n",
    "    \n",
    "    \n",
    "    \n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
