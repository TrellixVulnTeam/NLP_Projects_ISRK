{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bert Sentiment Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../../\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'interpret_nlp'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-39967ed87188>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtransformers\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mBertTokenizer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mmodules\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlrp_bert_modules\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mLRPBertForSequenceClassification\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mvisualization\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mheatmap\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mhtml_heatmap\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Documents\\GitHub\\Neural_Networks\\NLP\\clinicalBERT\\lrp_interpretation\\interpret_nlp\\modules\\lrp_bert_modules.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtransformers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbert\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmodeling_bert\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mbert\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0minterpret_nlp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mattribution\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlrp\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mlrp_linear\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlrp_matmul\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0minterpret_nlp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodules\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mbackprop_bert\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mbpbert\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0minterpret_nlp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodules\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackprop_bert\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mHiddenArray\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'interpret_nlp'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from IPython.core.display import display, HTML\n",
    "from transformers import BertTokenizer\n",
    "\n",
    "from modules.lrp_bert_modules import LRPBertForSequenceClassification\n",
    "\n",
    "from visualization.heatmap import html_heatmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'interpret_nlp.modules' has no attribute 'visualization'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-13-1af06d0363b4>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mnlpmodules\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvisualization\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m: module 'interpret_nlp.modules' has no attribute 'visualization'"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model...\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "# print(\"Loading model...\")\n",
    "# config_path = \"bert-sst-config.pt\"\n",
    "# state_dict_path = \"bert-sst.pt\"\n",
    "\n",
    "# tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "# model = LRPBertForSequenceClassification(torch.load(config_path))\n",
    "# model.load_state_dict(torch.load(state_dict_path))\n",
    "# model.eval()\n",
    "# print(\"Done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CIURRENTLY THIS IS WORKING WITH ORIGINAL MODEL FROM CLINICAL BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6d5a921778d840759ac4d9a7a54cc935",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Downloading'), FloatProgress(value=0.0, max=231508.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2e07675731a64257b965ee9d05d938a1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Downloading'), FloatProgress(value=0.0, max=28.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "79021c48f9554daca4d3649b17470e86",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Downloading'), FloatProgress(value=0.0, max=466062.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "config_path = \"./bert_config.json\"\n",
    "model_path = \"./pytorch_model.bin\"\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "model = LRPBertForSequenceClassification.from_pretrained(\"./\", num_labels = 1)\n",
    "# model = LRPBertForSequenceClassification.from_pretrained(torch.load(model_path))\n",
    "\n",
    "# model = torch.load(model_path)\n",
    "# print(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method Module.state_dict of LRPBertForSequenceClassification(\n",
       "  (bert): LRPBertModel(\n",
       "    (embeddings): LRPBertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LRPLayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): LRPBertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): LRPBertLayer(\n",
       "          (attention): LRPBertAttention(\n",
       "            (self): LRPBertSelfAttention(\n",
       "              (query): LRPLinear(in_features=768, out_features=768, bias=True)\n",
       "              (key): LRPLinear(in_features=768, out_features=768, bias=True)\n",
       "              (value): LRPLinear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): LRPBertSelfOutput(\n",
       "              (dense): LRPLinear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LRPLayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): LRPBertIntermediate(\n",
       "            (dense): LRPLinear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): LRPBertOutput(\n",
       "            (dense): LRPLinear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LRPLayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): LRPBertLayer(\n",
       "          (attention): LRPBertAttention(\n",
       "            (self): LRPBertSelfAttention(\n",
       "              (query): LRPLinear(in_features=768, out_features=768, bias=True)\n",
       "              (key): LRPLinear(in_features=768, out_features=768, bias=True)\n",
       "              (value): LRPLinear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): LRPBertSelfOutput(\n",
       "              (dense): LRPLinear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LRPLayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): LRPBertIntermediate(\n",
       "            (dense): LRPLinear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): LRPBertOutput(\n",
       "            (dense): LRPLinear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LRPLayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): LRPBertLayer(\n",
       "          (attention): LRPBertAttention(\n",
       "            (self): LRPBertSelfAttention(\n",
       "              (query): LRPLinear(in_features=768, out_features=768, bias=True)\n",
       "              (key): LRPLinear(in_features=768, out_features=768, bias=True)\n",
       "              (value): LRPLinear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): LRPBertSelfOutput(\n",
       "              (dense): LRPLinear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LRPLayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): LRPBertIntermediate(\n",
       "            (dense): LRPLinear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): LRPBertOutput(\n",
       "            (dense): LRPLinear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LRPLayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): LRPBertLayer(\n",
       "          (attention): LRPBertAttention(\n",
       "            (self): LRPBertSelfAttention(\n",
       "              (query): LRPLinear(in_features=768, out_features=768, bias=True)\n",
       "              (key): LRPLinear(in_features=768, out_features=768, bias=True)\n",
       "              (value): LRPLinear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): LRPBertSelfOutput(\n",
       "              (dense): LRPLinear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LRPLayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): LRPBertIntermediate(\n",
       "            (dense): LRPLinear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): LRPBertOutput(\n",
       "            (dense): LRPLinear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LRPLayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): LRPBertLayer(\n",
       "          (attention): LRPBertAttention(\n",
       "            (self): LRPBertSelfAttention(\n",
       "              (query): LRPLinear(in_features=768, out_features=768, bias=True)\n",
       "              (key): LRPLinear(in_features=768, out_features=768, bias=True)\n",
       "              (value): LRPLinear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): LRPBertSelfOutput(\n",
       "              (dense): LRPLinear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LRPLayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): LRPBertIntermediate(\n",
       "            (dense): LRPLinear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): LRPBertOutput(\n",
       "            (dense): LRPLinear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LRPLayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): LRPBertLayer(\n",
       "          (attention): LRPBertAttention(\n",
       "            (self): LRPBertSelfAttention(\n",
       "              (query): LRPLinear(in_features=768, out_features=768, bias=True)\n",
       "              (key): LRPLinear(in_features=768, out_features=768, bias=True)\n",
       "              (value): LRPLinear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): LRPBertSelfOutput(\n",
       "              (dense): LRPLinear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LRPLayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): LRPBertIntermediate(\n",
       "            (dense): LRPLinear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): LRPBertOutput(\n",
       "            (dense): LRPLinear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LRPLayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): LRPBertLayer(\n",
       "          (attention): LRPBertAttention(\n",
       "            (self): LRPBertSelfAttention(\n",
       "              (query): LRPLinear(in_features=768, out_features=768, bias=True)\n",
       "              (key): LRPLinear(in_features=768, out_features=768, bias=True)\n",
       "              (value): LRPLinear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): LRPBertSelfOutput(\n",
       "              (dense): LRPLinear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LRPLayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): LRPBertIntermediate(\n",
       "            (dense): LRPLinear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): LRPBertOutput(\n",
       "            (dense): LRPLinear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LRPLayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): LRPBertLayer(\n",
       "          (attention): LRPBertAttention(\n",
       "            (self): LRPBertSelfAttention(\n",
       "              (query): LRPLinear(in_features=768, out_features=768, bias=True)\n",
       "              (key): LRPLinear(in_features=768, out_features=768, bias=True)\n",
       "              (value): LRPLinear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): LRPBertSelfOutput(\n",
       "              (dense): LRPLinear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LRPLayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): LRPBertIntermediate(\n",
       "            (dense): LRPLinear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): LRPBertOutput(\n",
       "            (dense): LRPLinear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LRPLayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): LRPBertLayer(\n",
       "          (attention): LRPBertAttention(\n",
       "            (self): LRPBertSelfAttention(\n",
       "              (query): LRPLinear(in_features=768, out_features=768, bias=True)\n",
       "              (key): LRPLinear(in_features=768, out_features=768, bias=True)\n",
       "              (value): LRPLinear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): LRPBertSelfOutput(\n",
       "              (dense): LRPLinear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LRPLayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): LRPBertIntermediate(\n",
       "            (dense): LRPLinear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): LRPBertOutput(\n",
       "            (dense): LRPLinear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LRPLayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): LRPBertLayer(\n",
       "          (attention): LRPBertAttention(\n",
       "            (self): LRPBertSelfAttention(\n",
       "              (query): LRPLinear(in_features=768, out_features=768, bias=True)\n",
       "              (key): LRPLinear(in_features=768, out_features=768, bias=True)\n",
       "              (value): LRPLinear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): LRPBertSelfOutput(\n",
       "              (dense): LRPLinear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LRPLayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): LRPBertIntermediate(\n",
       "            (dense): LRPLinear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): LRPBertOutput(\n",
       "            (dense): LRPLinear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LRPLayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): LRPBertLayer(\n",
       "          (attention): LRPBertAttention(\n",
       "            (self): LRPBertSelfAttention(\n",
       "              (query): LRPLinear(in_features=768, out_features=768, bias=True)\n",
       "              (key): LRPLinear(in_features=768, out_features=768, bias=True)\n",
       "              (value): LRPLinear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): LRPBertSelfOutput(\n",
       "              (dense): LRPLinear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LRPLayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): LRPBertIntermediate(\n",
       "            (dense): LRPLinear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): LRPBertOutput(\n",
       "            (dense): LRPLinear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LRPLayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): LRPBertLayer(\n",
       "          (attention): LRPBertAttention(\n",
       "            (self): LRPBertSelfAttention(\n",
       "              (query): LRPLinear(in_features=768, out_features=768, bias=True)\n",
       "              (key): LRPLinear(in_features=768, out_features=768, bias=True)\n",
       "              (value): LRPLinear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): LRPBertSelfOutput(\n",
       "              (dense): LRPLinear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LRPLayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): LRPBertIntermediate(\n",
       "            (dense): LRPLinear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): LRPBertOutput(\n",
       "            (dense): LRPLinear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LRPLayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): LRPBertPooler(\n",
       "      (dense): LRPLinear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): LRPLinear(in_features=768, out_features=1, bias=True)\n",
       ")>"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set up example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_example = \"It's a lovely film with wonderful performances by Buy and \" \\\n",
    "#                \"Accorsi.\"\n",
    "\n",
    "test_example = ' he has experienced acute on chronic diastolic heart failure in the setting of volume overload due to his sepsis.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' he has experienced acute on chronic diastolic heart failure in the setting of volume overload due to his sepsis.'"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run normal forward pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8606415\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "inputs = tokenizer(test_example, return_tensors=\"pt\")\n",
    "logits = model(**inputs).logits.squeeze()\n",
    "\n",
    "m = nn.Sigmoid()\n",
    "logits = torch.squeeze(m(logits)).detach().cpu().numpy()\n",
    "\n",
    "print(logits)\n",
    "# classes = [\"<unk>\", \"positive\", \"negative\", \"neutral\"]\n",
    "classes = [\"not_admitted\", \"admitted\"]\n",
    "\n",
    "# print(\"Logit Scores:\")\n",
    "# for c, score in zip(classes, logits):\n",
    "#     print(\"{}: {}\".format(c, score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run attribution forward pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attr Forward Pass Output:\n",
      "[[1.8206284]]\n"
     ]
    }
   ],
   "source": [
    "inputs = tokenizer(test_example, return_tensors=\"pt\")\n",
    "model.attr()\n",
    "output = model(**inputs)\n",
    "\n",
    "print(\"Attr Forward Pass Output:\")\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run LRP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LRP Scores:\n",
      "he: 0.023229010748793775\n",
      "has: 0.009376843853490531\n",
      "experienced: 0.0030572293401592556\n",
      "acute: 0.016233898386067237\n",
      "on: 0.0022083858891211484\n",
      "chronic: 0.024892846885231558\n",
      "dia: 0.0006336948378041895\n",
      "##sto: 0.006915873119135614\n",
      "##lic: 0.005015703820084047\n",
      "heart: 0.017993832390379483\n",
      "failure: 0.0218779687720887\n",
      "in: -0.0008363484028907171\n",
      "the: 0.002043464432778934\n",
      "setting: -0.017114741092442196\n",
      "of: -0.005795778446079742\n",
      "volume: 0.00019336913559048446\n",
      "over: 0.003116229516476183\n",
      "##load: -0.013129035339667248\n",
      "due: 0.024921277787359534\n",
      "to: 0.004506036147102576\n",
      "his: -0.011142785601670976\n",
      "sep: 0.0022926451238911\n",
      "##sis: 0.033563854029015266\n",
      ".: 0.0018885693068613082\n",
      "Relevance of word embeddings:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:#f3c6b0;font-weight:bold;background-color: #ffffff;padding-top: 15px;padding-bottom: 15px;\"> he </span> <span style=\"color:#efcdbb;font-weight:bold;background-color: #ffffff;padding-top: 15px;padding-bottom: 15px;\"> has </span> <span style=\"color:#f1cbb8;font-weight:bold;background-color: #ffffff;padding-top: 15px;padding-bottom: 15px;\"> experienced </span> <span style=\"color:#d2dae7;font-weight:bold;background-color: #ffffff;padding-top: 15px;padding-bottom: 15px;\"> acute </span> <span style=\"color:#e5d8d0;font-weight:bold;background-color: #ffffff;padding-top: 15px;padding-bottom: 15px;\"> on </span> <span style=\"color:#d65243;font-weight:bold;background-color: #ffffff;padding-top: 15px;padding-bottom: 15px;\"> chronic </span> <span style=\"color:#d8dbe1;font-weight:bold;background-color: #ffffff;padding-top: 15px;padding-bottom: 15px;\"> dia </span> <span style=\"color:#f5bfa5;font-weight:bold;background-color: #ffffff;padding-top: 15px;padding-bottom: 15px;\"> ##sto </span> <span style=\"color:#e0dad7;font-weight:bold;background-color: #ffffff;padding-top: 15px;padding-bottom: 15px;\"> ##lic </span> <span style=\"color:#efcdbb;font-weight:bold;background-color: #ffffff;padding-top: 15px;padding-bottom: 15px;\"> heart </span> <span style=\"color:#dfdbd9;font-weight:bold;background-color: #ffffff;padding-top: 15px;padding-bottom: 15px;\"> failure </span> <span style=\"color:#d9dce0;font-weight:bold;background-color: #ffffff;padding-top: 15px;padding-bottom: 15px;\"> in </span> <span style=\"color:#4c66d6;font-weight:bold;background-color: #ffffff;padding-top: 15px;padding-bottom: 15px;\"> the </span> <span style=\"color:#85a8fb;font-weight:bold;background-color: #ffffff;padding-top: 15px;padding-bottom: 15px;\"> setting </span> <span style=\"color:#e1dad6;font-weight:bold;background-color: #ffffff;padding-top: 15px;padding-bottom: 15px;\"> of </span> <span style=\"color:#e8d5ca;font-weight:bold;background-color: #ffffff;padding-top: 15px;padding-bottom: 15px;\"> volume </span> <span style=\"color:#d9dce0;font-weight:bold;background-color: #ffffff;padding-top: 15px;padding-bottom: 15px;\"> over </span> <span style=\"color:#e8d5ca;font-weight:bold;background-color: #ffffff;padding-top: 15px;padding-bottom: 15px;\"> ##load </span> <span style=\"color:#f6a384;font-weight:bold;background-color: #ffffff;padding-top: 15px;padding-bottom: 15px;\"> due </span> <span style=\"color:#e2d9d4;font-weight:bold;background-color: #ffffff;padding-top: 15px;padding-bottom: 15px;\"> to </span> <span style=\"color:#ced9eb;font-weight:bold;background-color: #ffffff;padding-top: 15px;padding-bottom: 15px;\"> his </span> <span style=\"color:#f49d7e;font-weight:bold;background-color: #ffffff;padding-top: 15px;padding-bottom: 15px;\"> sep </span> <span style=\"color:#b30326;font-weight:bold;background-color: #ffffff;padding-top: 15px;padding-bottom: 15px;\"> ##sis </span> <span style=\"color:#e3d9d3;font-weight:bold;background-color: #ffffff;padding-top: 15px;padding-bottom: 15px;\"> . </span> "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Relevance of positional embeddings:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:#b30326;font-weight:bold;background-color: #ffffff;padding-top: 15px;padding-bottom: 15px;\"> he </span> <span style=\"color:#d7dbe2;font-weight:bold;background-color: #ffffff;padding-top: 15px;padding-bottom: 15px;\"> has </span> <span style=\"color:#e0dad7;font-weight:bold;background-color: #ffffff;padding-top: 15px;padding-bottom: 15px;\"> experienced </span> <span style=\"color:#f1cab6;font-weight:bold;background-color: #ffffff;padding-top: 15px;padding-bottom: 15px;\"> acute </span> <span style=\"color:#f5c1a8;font-weight:bold;background-color: #ffffff;padding-top: 15px;padding-bottom: 15px;\"> on </span> <span style=\"color:#89acfc;font-weight:bold;background-color: #ffffff;padding-top: 15px;padding-bottom: 15px;\"> chronic </span> <span style=\"color:#d6dbe4;font-weight:bold;background-color: #ffffff;padding-top: 15px;padding-bottom: 15px;\"> dia </span> <span style=\"color:#f4c2aa;font-weight:bold;background-color: #ffffff;padding-top: 15px;padding-bottom: 15px;\"> ##sto </span> <span style=\"color:#b3ccfa;font-weight:bold;background-color: #ffffff;padding-top: 15px;padding-bottom: 15px;\"> ##lic </span> <span style=\"color:#a3c1fe;font-weight:bold;background-color: #ffffff;padding-top: 15px;padding-bottom: 15px;\"> heart </span> <span style=\"color:#d8dbe1;font-weight:bold;background-color: #ffffff;padding-top: 15px;padding-bottom: 15px;\"> failure </span> <span style=\"color:#e5d8d0;font-weight:bold;background-color: #ffffff;padding-top: 15px;padding-bottom: 15px;\"> in </span> <span style=\"color:#d6dbe4;font-weight:bold;background-color: #ffffff;padding-top: 15px;padding-bottom: 15px;\"> the </span> <span style=\"color:#ced9eb;font-weight:bold;background-color: #ffffff;padding-top: 15px;padding-bottom: 15px;\"> setting </span> <span style=\"color:#d3dbe6;font-weight:bold;background-color: #ffffff;padding-top: 15px;padding-bottom: 15px;\"> of </span> <span style=\"color:#e7d6cc;font-weight:bold;background-color: #ffffff;padding-top: 15px;padding-bottom: 15px;\"> volume </span> <span style=\"color:#dedbda;font-weight:bold;background-color: #ffffff;padding-top: 15px;padding-bottom: 15px;\"> over </span> <span style=\"color:#b2cbfb;font-weight:bold;background-color: #ffffff;padding-top: 15px;padding-bottom: 15px;\"> ##load </span> <span style=\"color:#cad8ee;font-weight:bold;background-color: #ffffff;padding-top: 15px;padding-bottom: 15px;\"> due </span> <span style=\"color:#f2c9b5;font-weight:bold;background-color: #ffffff;padding-top: 15px;padding-bottom: 15px;\"> to </span> <span style=\"color:#91b3fe;font-weight:bold;background-color: #ffffff;padding-top: 15px;padding-bottom: 15px;\"> his </span> <span style=\"color:#5c7be5;font-weight:bold;background-color: #ffffff;padding-top: 15px;padding-bottom: 15px;\"> sep </span> <span style=\"color:#dcdcdd;font-weight:bold;background-color: #ffffff;padding-top: 15px;padding-bottom: 15px;\"> ##sis </span> <span style=\"color:#e1dad6;font-weight:bold;background-color: #ffffff;padding-top: 15px;padding-bottom: 15px;\"> . </span> "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Relevance of type embeddings:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:#cdd9ec;font-weight:bold;background-color: #ffffff;padding-top: 15px;padding-bottom: 15px;\"> he </span> <span style=\"color:#e2d9d4;font-weight:bold;background-color: #ffffff;padding-top: 15px;padding-bottom: 15px;\"> has </span> <span style=\"color:#d1dae8;font-weight:bold;background-color: #ffffff;padding-top: 15px;padding-bottom: 15px;\"> experienced </span> <span style=\"color:#a6c3fd;font-weight:bold;background-color: #ffffff;padding-top: 15px;padding-bottom: 15px;\"> acute </span> <span style=\"color:#c5d5f2;font-weight:bold;background-color: #ffffff;padding-top: 15px;padding-bottom: 15px;\"> on </span> <span style=\"color:#b0cbfb;font-weight:bold;background-color: #ffffff;padding-top: 15px;padding-bottom: 15px;\"> chronic </span> <span style=\"color:#ebd3c6;font-weight:bold;background-color: #ffffff;padding-top: 15px;padding-bottom: 15px;\"> dia </span> <span style=\"color:#dbdcde;font-weight:bold;background-color: #ffffff;padding-top: 15px;padding-bottom: 15px;\"> ##sto </span> <span style=\"color:#dbdcde;font-weight:bold;background-color: #ffffff;padding-top: 15px;padding-bottom: 15px;\"> ##lic </span> <span style=\"color:#f1cab6;font-weight:bold;background-color: #ffffff;padding-top: 15px;padding-bottom: 15px;\"> heart </span> <span style=\"color:#dddcdb;font-weight:bold;background-color: #ffffff;padding-top: 15px;padding-bottom: 15px;\"> failure </span> <span style=\"color:#d7dbe2;font-weight:bold;background-color: #ffffff;padding-top: 15px;padding-bottom: 15px;\"> in </span> <span style=\"color:#8eb1fd;font-weight:bold;background-color: #ffffff;padding-top: 15px;padding-bottom: 15px;\"> the </span> <span style=\"color:#d9dce0;font-weight:bold;background-color: #ffffff;padding-top: 15px;padding-bottom: 15px;\"> setting </span> <span style=\"color:#c6d6f1;font-weight:bold;background-color: #ffffff;padding-top: 15px;padding-bottom: 15px;\"> of </span> <span style=\"color:#e7d6cc;font-weight:bold;background-color: #ffffff;padding-top: 15px;padding-bottom: 15px;\"> volume </span> <span style=\"color:#edd0c1;font-weight:bold;background-color: #ffffff;padding-top: 15px;padding-bottom: 15px;\"> over </span> <span style=\"color:#c3d5f2;font-weight:bold;background-color: #ffffff;padding-top: 15px;padding-bottom: 15px;\"> ##load </span> <span style=\"color:#d9dce0;font-weight:bold;background-color: #ffffff;padding-top: 15px;padding-bottom: 15px;\"> due </span> <span style=\"color:#3a4cc0;font-weight:bold;background-color: #ffffff;padding-top: 15px;padding-bottom: 15px;\"> to </span> <span style=\"color:#a4c2fe;font-weight:bold;background-color: #ffffff;padding-top: 15px;padding-bottom: 15px;\"> his </span> <span style=\"color:#f2c7b2;font-weight:bold;background-color: #ffffff;padding-top: 15px;padding-bottom: 15px;\"> sep </span> <span style=\"color:#6485eb;font-weight:bold;background-color: #ffffff;padding-top: 15px;padding-bottom: 15px;\"> ##sis </span> <span style=\"color:#d2dae7;font-weight:bold;background-color: #ffffff;padding-top: 15px;padding-bottom: 15px;\"> . </span> "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Relevance of combined embeddings:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:#e46e56;font-weight:bold;background-color: #ffffff;padding-top: 15px;padding-bottom: 15px;\"> he </span> <span style=\"color:#f5c0a7;font-weight:bold;background-color: #ffffff;padding-top: 15px;padding-bottom: 15px;\"> has </span> <span style=\"color:#e7d6cc;font-weight:bold;background-color: #ffffff;padding-top: 15px;padding-bottom: 15px;\"> experienced </span> <span style=\"color:#f49d7e;font-weight:bold;background-color: #ffffff;padding-top: 15px;padding-bottom: 15px;\"> acute </span> <span style=\"color:#e5d8d0;font-weight:bold;background-color: #ffffff;padding-top: 15px;padding-bottom: 15px;\"> on </span> <span style=\"color:#de624e;font-weight:bold;background-color: #ffffff;padding-top: 15px;padding-bottom: 15px;\"> chronic </span> <span style=\"color:#dfdbd9;font-weight:bold;background-color: #ffffff;padding-top: 15px;padding-bottom: 15px;\"> dia </span> <span style=\"color:#f2c9b5;font-weight:bold;background-color: #ffffff;padding-top: 15px;padding-bottom: 15px;\"> ##sto </span> <span style=\"color:#edcfc0;font-weight:bold;background-color: #ffffff;padding-top: 15px;padding-bottom: 15px;\"> ##lic </span> <span style=\"color:#f29173;font-weight:bold;background-color: #ffffff;padding-top: 15px;padding-bottom: 15px;\"> heart </span> <span style=\"color:#e8775d;font-weight:bold;background-color: #ffffff;padding-top: 15px;padding-bottom: 15px;\"> failure </span> <span style=\"color:#d9dce0;font-weight:bold;background-color: #ffffff;padding-top: 15px;padding-bottom: 15px;\"> in </span> <span style=\"color:#e4d8d1;font-weight:bold;background-color: #ffffff;padding-top: 15px;padding-bottom: 15px;\"> the </span> <span style=\"color:#8aadfd;font-weight:bold;background-color: #ffffff;padding-top: 15px;padding-bottom: 15px;\"> setting </span> <span style=\"color:#c3d5f2;font-weight:bold;background-color: #ffffff;padding-top: 15px;padding-bottom: 15px;\"> of </span> <span style=\"color:#dddcdb;font-weight:bold;background-color: #ffffff;padding-top: 15px;padding-bottom: 15px;\"> volume </span> <span style=\"color:#e7d6cc;font-weight:bold;background-color: #ffffff;padding-top: 15px;padding-bottom: 15px;\"> over </span> <span style=\"color:#9fbefe;font-weight:bold;background-color: #ffffff;padding-top: 15px;padding-bottom: 15px;\"> ##load </span> <span style=\"color:#dd604c;font-weight:bold;background-color: #ffffff;padding-top: 15px;padding-bottom: 15px;\"> due </span> <span style=\"color:#ecd1c3;font-weight:bold;background-color: #ffffff;padding-top: 15px;padding-bottom: 15px;\"> to </span> <span style=\"color:#aac6fd;font-weight:bold;background-color: #ffffff;padding-top: 15px;padding-bottom: 15px;\"> his </span> <span style=\"color:#e5d8d0;font-weight:bold;background-color: #ffffff;padding-top: 15px;padding-bottom: 15px;\"> sep </span> <span style=\"color:#b30326;font-weight:bold;background-color: #ffffff;padding-top: 15px;padding-bottom: 15px;\"> ##sis </span> <span style=\"color:#e4d8d1;font-weight:bold;background-color: #ffffff;padding-top: 15px;padding-bottom: 15px;\"> . </span> "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokens = tokenizer.tokenize(test_example)\n",
    "rel_y = np.zeros(output.shape)\n",
    "rel_y[:, 0] = output[:, 0]\n",
    "rel_word, rel_pos, rel_type, rel_embed = model.attr_backward(rel_y, eps=.1)\n",
    "rel_word = np.sum(rel_word[0, 1:-1], -1)\n",
    "rel_pos = np.sum(rel_pos[0, 1:-1], -1)\n",
    "rel_type = np.sum(rel_type[0, 1:-1], -1)\n",
    "rel_embed = np.sum(rel_embed[0, 1:-1], -1)\n",
    "\n",
    "print(\"LRP Scores:\")\n",
    "for t, s in zip(tokens, rel_embed):\n",
    "    print(t, s, sep=\": \")\n",
    "    \n",
    "print(\"Relevance of word embeddings:\")\n",
    "display(HTML(html_heatmap(tokens, list(rel_word))))\n",
    "\n",
    "print(\"Relevance of positional embeddings:\")\n",
    "display(HTML(html_heatmap(tokens, list(rel_pos))))\n",
    "\n",
    "print(\"Relevance of type embeddings:\")\n",
    "display(HTML(html_heatmap(tokens, list(rel_type))))\n",
    "\n",
    "print(\"Relevance of combined embeddings:\")\n",
    "display(HTML(html_heatmap(tokens, list(rel_embed))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "index 1 is out of bounds for axis 1 with size 1",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-55-52e7277c4f89>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mrel_y\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m: index 1 is out of bounds for axis 1 with size 1"
     ]
    }
   ],
   "source": [
    "def get_LRP_derived_token_scores():\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
